{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflows Deep Dive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> \"A Quote by Someone.\" ~ Some O Ne"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Welcome to PyData and to the workshop \"Workflows Deep Dive: From Data Engineering to Machine Learning.\"\n",
    "\n",
    "Today we will learn about different data workflows and about how to create one for ourselves. In particular, we'll go through the following series of steps to create a full blown data project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Motivation\n",
    "2. What Are Workflows?\n",
    "3. Our Project for Today\n",
    "4. Setting Up a Monorepo\n",
    "5. Data Engineering\n",
    "6. Analytics\n",
    "7. Data Science\n",
    "8. Beyond Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Motivation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![meme](https://debate.protocommunications.com/wp-content/uploads/2018/03/frustrated-meme.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. What Are Workflows?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![workflow](https://assets.website-files.com/634681057b887c6f4830fae2/6367ddcfcb0f6802bc761e5e_62e988200820e095735be5e3_Workflows.png)\n",
    "\n",
    "> A series of steps to get things done."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Our Project for Today"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wireless-chemistry",
   "metadata": {},
   "source": [
    "![img](https://images.unsplash.com/photo-1578948667675-74f499f141f7?ixlib=rb-4.0.3&ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&auto=format&fit=crop&w=1740&q=80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are a data analyst working at Beautiful Analytics, and you have been given a project in which you will work using data generated from shared bikes systems in the citieSeoul (South Korea). Your customer is the government of the nation and what they want you to solve is,\n",
    "\n",
    "**Challenge #1**\n",
    "\n",
    "> To predict/forecast how many bikes they will to need to have available in the city at every hour of the dat for the next few years?\n",
    "\n",
    "The government captures similar data but, as you can imagine, they all use different words and measure similar variables in different ways. This means that our first job before we can answer the question above is to fix the data and put it in a more user-friendly way. While we are at it, we should also try and automate our pipeline so that the next time we need to read, transform, and load new versions of all the data sources for this project, we could do so with the click of a button rather than having to write everything again from scratch. So our first real problem is,\n",
    "\n",
    "**Challenge #0**\n",
    "\n",
    "> Create a data pipeline that extracts, transforms and loads the necessary data for the task at hand."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Setting Up a Monorepo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![monorepo](https://miro.medium.com/max/671/1*jXZ26bo8TQE1Q0RBMan8kQ.jpeg)\n",
    "\n",
    "What is a Monorepo?\n",
    "\n",
    "A \"Mono\" (single) \"Repo\" (repository) is a software development strategy where the code for multiple interrelated projects is stored in single repository.\n",
    "\n",
    "While this strategy is common practice in the software development, it can be considered a relatively new one in the data profession.\n",
    "\n",
    "Before we talk about why would we want to use one, it is important to highlight that a `monorepo != monolith` application. While the former can be used to build a monolithic application, the latter can still be using different repositories.\n",
    "\n",
    "Why use one?\n",
    "- You want develop multiple project in one place.\n",
    "- You want to increase code visibility and reusability.\n",
    "- You want to improve code refactoring.\n",
    "- You want to standardize best practices.\n",
    "- You want to reduce the cost of switching from one project to another.\n",
    "- ...\n",
    "\n",
    "Why not use one?\n",
    "- Changing one piece of code could potentially affect a larger system.\n",
    "- Your deployment process could take longer.\n",
    "- Everyone would carry a full copy of everything in their laptops.\n",
    "- ...\n",
    "\n",
    "Let's get started?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Engineering as a discipline is the backbone of any data endeavor undertaken by small to large organization. It is the discipline in charge of creating the data flows and infrastructure of organizations so that everyone can take advantage of data.\n",
    "\n",
    "In this scenario, taking advantage of data might include creating data lakes, warehouses, and pipelines that move data between the former two and the processes and applications or systems that produce them. \n",
    "\n",
    "While it is totally possible to extract value from data without having any data engineering capabilities, having at team of these players can improve the data capabilities of any company by orders of magnitude. In fact, the value provided by data engineers can be seen through the rise of job ads for such role on LinkedIn, Indeed, Seek, and many others.\n",
    "\n",
    "That said, engineering data systems requires skills and tools, and since I know you have the former under your belt, lets talk about the latter. For our project, we will be using [Dagster](), a data orchestration tool built for data and machine learning engineers.\n",
    "\n",
    "Dagster has a few important concepts to learn before one can be productive with it.\n",
    "- `assets`\n",
    "- `op`s\n",
    "- `job`s\n",
    "- `graph`s\n",
    "- `repository`\n",
    "- `workspace`\n",
    "- `schedule` and `sensors`\n",
    "- `dagit`\n",
    "\n",
    "Let's get started by creating a project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dagster project scaffold --name data_eng"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at our project.\n",
    "\n",
    "```bash\n",
    "cd data_eng\n",
    "\n",
    "tree .\n",
    "```\n",
    "\n",
    "```\n",
    "data_eng\n",
    "â”œâ”€â”€ data_eng\n",
    "â”‚   â”œâ”€â”€ assets\n",
    "â”‚   â”‚   â””â”€â”€ __init__.py\n",
    "â”‚   â”œâ”€â”€ __init__.py\n",
    "â”‚   â””â”€â”€ repository.py\n",
    "â”œâ”€â”€ data_eng_tests\n",
    "â”‚   â”œâ”€â”€ __init__.py\n",
    "â”‚   â””â”€â”€ test_assets.py\n",
    "â”œâ”€â”€ pyproject.toml\n",
    "â”œâ”€â”€ README.md\n",
    "â”œâ”€â”€ setup.cfg\n",
    "â”œâ”€â”€ setup.py\n",
    "â””â”€â”€ workspace.yaml\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now create an environment or activate one that we have already created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# conda or mamba\n",
    "# !conda create -n data_eng python=3.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -e \".[dev]\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is time to get started creating some flows.\n",
    "\n",
    "Before we get the data, let's talk about the mechanics of a job with an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir data_eng/data_eng/assets/example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile data_eng/data_eng/hello_assets.py\n",
    "\n",
    "import pandas as pd\n",
    "from dagster import get_dagster_logger, asset\n",
    "from pathlib import Path\n",
    "\n",
    "path = Path().cwd().parent/\"data/example\"\n",
    "\n",
    "@asset\n",
    "def get_data():\n",
    "    return pd.read_csv(path/\"bike_sharing_hourly.csv\")\n",
    "\n",
    "@asset\n",
    "def group_it(get_data):\n",
    "    smol_data = get_data.groupby(\"weekday\")[\"cnt\"].sum()\n",
    "    get_dagster_logger().info(f\"Smol Data:\\n{smol_data.to_markdown()}\")\n",
    "    return smol_data\n",
    "\n",
    "@asset\n",
    "def save_it(group_it):\n",
    "    group_it.to_frame().to_parquet(path/\"hello2.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have created our first few assets. Let's walk through them in the file we just created and then in the UI by running the following command.\n",
    "\n",
    "```bash\n",
    "dagit -f data_eng/hello_assets.py\n",
    "```\n",
    "\n",
    "You can go to `http://127.0.0.1:3000`.\n",
    "\n",
    "Next, let's go over `op`s and `job`s to get a gist of what we can accomplish with them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir data_eng/data_eng/jobs\n",
    "touch data_eng/data_eng/jobs/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile data_eng/data_eng/jobs/seoul_jobs.py\n",
    "\n",
    "import pandas as pd\n",
    "import re\n",
    "from pathlib import Path\n",
    "from dagster import op, job, get_dagster_logger\n",
    "\n",
    "\n",
    "@op\n",
    "def seoul_data() -> pd.DataFrame:\n",
    "    url = 'https://archive.ics.uci.edu/ml/machine-learning-databases/00560/SeoulBikeData.csv'\n",
    "    path = Path().cwd().parent/\"data/raw\"\n",
    "    file_name = 'SeoulBikeData.csv'\n",
    "    data = pd.read_csv(url, encoding='iso-8859-1')\n",
    "    data.to_csv(path/file_name, index=False, encoding=\"UTF-8\")\n",
    "    return data\n",
    "\n",
    "@op\n",
    "def seoul_has_new_col_names(seoul_data) -> pd.DataFrame:\n",
    "    new_cols = [re.sub(r'[^a-zA-Z0-9\\s]', '', col).lower().replace(r\" \", \"_\") for col in seoul_data.columns]\n",
    "    seoul_data.columns = new_cols\n",
    "    return seoul_data\n",
    "\n",
    "@op\n",
    "def seoul_w_new_dates(seoul_has_new_col_names) -> pd.DataFrame:\n",
    "    data = seoul_has_new_col_names.copy()\n",
    "    data['date'] = pd.to_datetime(data['date'], format=\"%d/%m/%Y\")\n",
    "    data.sort_values(['date', 'hour'], inplace=True)\n",
    "    data[\"year\"] = data['date'].dt.year\n",
    "    data[\"month\"] = data['date'].dt.month\n",
    "    data[\"week\"] = data['date'].dt.isocalendar().week\n",
    "    data[\"day\"] = data['date'].dt.day\n",
    "    data[\"day_of_week\"] = data['date'].dt.dayofweek\n",
    "    data[\"day_of_year\"] = data['date'].dt.dayofyear\n",
    "    data[\"is_month_end\"] = data['date'].dt.is_month_end\n",
    "    data[\"is_month_start\"] = data['date'].dt.is_month_start\n",
    "    data[\"is_quarter_end\"] = data['date'].dt.is_quarter_end\n",
    "    data[\"is_quarter_start\"] = data['date'].dt.is_quarter_start\n",
    "    data[\"is_year_end\"] = data['date'].dt.is_year_end\n",
    "    data[\"is_year_start\"] = data['date'].dt.is_year_start\n",
    "    data.drop('date', axis=1, inplace=True)\n",
    "\n",
    "    return data\n",
    "\n",
    "@op\n",
    "def seoul_w_dummies(seoul_w_new_dates):\n",
    "    return pd.get_dummies(data=seoul_w_new_dates, columns=['holiday', 'seasons', 'functioning_day'])\n",
    "\n",
    "@op\n",
    "def save_interim_data(seoul_w_dummies):\n",
    "    data_path = Path().cwd().parent/\"data/interim\"\n",
    "    file_name = \"clean_data.parquet\"\n",
    "\n",
    "    if not data_path.exists(): data_path.mkdir(parents=True)\n",
    "    seoul_w_dummies.to_parquet(data_path.joinpath(file_name), compression=\"snappy\")\n",
    "\n",
    "@op\n",
    "def split_and_save_final_time_series_data(seoul_w_dummies):\n",
    "    full_df = seoul_w_dummies.copy()\n",
    "    split_pct = 0.30\n",
    "    data_path = Path().cwd().parent/\"data/processed\"\n",
    "    train_file_name = \"train.parquet\"\n",
    "    test_file_name = \"test.parquet\"\n",
    "\n",
    "    n_train = int(len(full_df) - len(full_df) * split_pct)\n",
    "\n",
    "    if not data_path.exists():\n",
    "        data_path.mkdir(parents=True)\n",
    "\n",
    "    full_df[:n_train].reset_index(drop=True).to_parquet(\n",
    "        data_path.joinpath(train_file_name), compression=\"snappy\"\n",
    "    )\n",
    "    full_df[n_train:].reset_index(drop=True).to_parquet(\n",
    "        data_path.joinpath(test_file_name), compression=\"snappy\"\n",
    "    )\n",
    "\n",
    "    get_dagster_logger().info(f\"File Partitioned Successfully!\")\n",
    "\n",
    "@job\n",
    "def seoul_pipeline():\n",
    "\n",
    "    data = seoul_data()\n",
    "    data_w_new_col_names = seoul_has_new_col_names(data)\n",
    "    data_w_new_vars = seoul_w_new_dates(data_w_new_col_names)\n",
    "    data_w_dummies = seoul_w_dummies(data_w_new_vars)\n",
    "\n",
    "    save_interim_data(data_w_new_vars)\n",
    "    split_and_save_final_time_series_data(data_w_dummies)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the same command from before but with our new file, let's evaluate our jobs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!dagit -f data_eng/jobs/seoul_jobs.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise\n",
    "\n",
    "Create a similar job with the `op` and the `job` classes and\n",
    "1. Create a groupby object and take the average count of bikes rented per month.\n",
    "2. Log the max value in the result.\n",
    "3. Run the job in the dagit UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile data_eng/data_eng/exercise_jobs.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "from dagster import get_dagster_logger, job, op\n",
    "from pathlib import Path\n",
    "\n",
    "path = Path().cwd()/\"data/example\"\n",
    "\n",
    "@op\n",
    "def get_data():\n",
    "    return pd.read_csv(data_path/\"bike_sharing_hourly.csv\")\n",
    "\n",
    "@op\n",
    "def group_it(get_data):\n",
    "    grouped = ___\n",
    "    max_value = ___\n",
    "    get_dagster_logger().info(f\"Max data:\\n{___}\")\n",
    "\n",
    "    return grouped\n",
    "\n",
    "@op\n",
    "def save_it(group_it):\n",
    "    group_it.to_frame().to_parquet(path/\"hello1.parquet\")\n",
    "\n",
    "@job\n",
    "def bike_stats():\n",
    "    data = ___\n",
    "    group = ___\n",
    "    save_it(group)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we wanted to add a schedule, we would add\n",
    "\n",
    "```python\n",
    "bike_schedule = ScheduleDefinition(job=seoul_pipeline, cron_schedule=\"0 0 * * *\")\n",
    "\n",
    "@sensor(job=bike_schedule)\n",
    "def job2_sensor():\n",
    "    should_run = True\n",
    "    if should_run:\n",
    "        yield RunRequest(run_key=None, run_config={})\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's examine a machine learning workflow with dagster where our data, models, and metrics would all be considered software defined assets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile data_eng/data_eng/assets/__init__.py\n",
    "\n",
    "from dagster import asset, get_dagster_logger\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn import metrics as mtr\n",
    "\n",
    "\n",
    "@asset#(group_name=\"ml_team\")\n",
    "def train_data():\n",
    "    path = Path().cwd().parent/\"data/processed/train.parquet\"\n",
    "    return pd.read_parquet(path)\n",
    "    \n",
    "\n",
    "@asset#(group_name=\"ml_team\")\n",
    "def test_data():\n",
    "    path = Path().cwd().parent/\"data/processed/test.parquet\"\n",
    "    return pd.read_parquet(path)\n",
    "\n",
    "@asset#(group_name=\"ml_team\")\n",
    "def rf_model(train_data):\n",
    "    X_train = train_data.drop('rented_bike_count', axis=1)\n",
    "    y_train = train_data['rented_bike_count']\n",
    "    \n",
    "    rf = RandomForestRegressor(\n",
    "        n_estimators=200, max_features=0.2, min_samples_leaf=1, verbose=1,\n",
    "        random_state=42, n_jobs=-1, oob_score=True\n",
    "    )\n",
    "\n",
    "    rf.fit(X_train, y_train)\n",
    "    \n",
    "    return rf\n",
    "\n",
    "@asset#(group_name=\"ml_team\")\n",
    "def metrics(rf_model, test_data):\n",
    "    X_test = test_data.drop('rented_bike_count', axis=1)\n",
    "    y_test = test_data['rented_bike_count']\n",
    "\n",
    "    predictions = rf_model.predict(X_test.values)\n",
    "\n",
    "    mae = mtr.mean_absolute_error(y_test.values, predictions)\n",
    "    rmse = np.sqrt(mtr.mean_squared_error(y_test.values, predictions))\n",
    "    r2_score = rf_model.score(X_test.values, y_test.values)\n",
    "\n",
    "    our_metrics = pd.DataFrame({\"MAE\": mae, \"RMSE\": rmse, \"R^2\": r2_score}, index=[0])\n",
    "    get_dagster_logger().info(f\"Our Metrics:\\n{our_metrics.to_markdown()}\")\n",
    "\n",
    "    return our_metrics.to_markdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-preliminary",
   "metadata": {},
   "source": [
    "> \"Above all else show the data.\" ~ Edward Tufte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we start extracting value from data, let's set up our directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "mkdir analytics analytics/apps analytics/notebooks analytics/reports\n",
    "touch analytics/README.md"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4f61248-c560-4297-aaa4-530b9b155f71",
   "metadata": {},
   "source": [
    "### Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a3b365a-80ee-4ad8-bd59-9c2e170da5b5",
   "metadata": {},
   "source": [
    "These are the tools we will use throughout this section, they will help us make the most out of our data to derive some insights and share our results with others. The summary for each library was taken directly from their respective website, and you can go to those websites by clicking on their names.\n",
    "\n",
    "- [pandas](https://pandas.pydata.org/)\n",
    "\n",
    "> \"pandas is a fast, powerful, flexible and easy to use open source data analysis and manipulation tool,\n",
    "built on top of the Python programming language.\"\n",
    "\n",
    "- [HoloViews](https://holoviews.org/)\n",
    "\n",
    "> \"HoloViews is an open-source Python library designed to make data analysis and visualization seamless and simple. With HoloViews, you can usually express what you want to do in very few lines of code, letting you focus on what you are trying to explore and convey, not on the process of plotting.\"\n",
    "\n",
    "- [Panel](https://panel.holoviz.org/)\n",
    "\n",
    "> \"Panel is an open-source Python library that lets you create custom interactive web apps and dashboards by connecting user-defined widgets to plots, images, tables, or text.\"\n",
    "\n",
    "Let's get started by importing these packages and a few others."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intensive-lebanon",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np, os\n",
    "import matplotlib.pyplot as plt\n",
    "import holoviews as hv, panel as pn\n",
    "from holoviews import dim, opts\n",
    "# import geopandas as gpd, geoviews as gv\n",
    "from holoviews.element import tiles\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "hv.extension('bokeh', 'matplotlib')\n",
    "pn.extension()\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "pd.options.display.max_columns = None\n",
    "pd.options.display.max_rows = None\n",
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b7b562b-3e16-4a33-94bb-97375bee380e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path().cwd().parent.joinpath(\"data\", \"interim\", \"clean_data.parquet\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eight-blood",
   "metadata": {},
   "source": [
    "### Seoul Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abandoned-buyer",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(path)\n",
    "print(df.shape)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "functioning-techno",
   "metadata": {},
   "source": [
    "## Dashboard"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unlike-microwave",
   "metadata": {},
   "source": [
    "This section is all about building a dashboard to showcase some key metrics from our dataset. We will start with the widgets, the tools that provide us with interactivity, and work our way upwards until we get to the final output."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acute-democracy",
   "metadata": {},
   "source": [
    "### 5.1 The Widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "graduate-theme",
   "metadata": {},
   "source": [
    "Panel contains plenty of widgets for us from a function called `interact` and another called `widgets`. The former provides interactive capabilities to user-defined functions and that in turn allows us to parametrise the arguments we pass into different plotting methods. The latter creates a user interface as tiny as input box and as complex as a full web application.\n",
    "\n",
    "Both tools provide a fine control on the interactivity behind the scenes of a complex web application. In some instances, we can also access some of the JavaScript and CSS code running behind the scenes to customise our applications even further.\n",
    "\n",
    "One thing we might care about, depending on how many people we might be traveling with, is the kind of property we want to rent. If we go on a family trip, a big house, villa, or condo might suffice. If we go on our own, a hostel will be a great choice to meet people and to quickly find things to do. That said, let's first create a list will all of the kinds of properties in our dataset, and then check out how panles' `widgets` work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "compliant-banana",
   "metadata": {},
   "outputs": [],
   "source": [
    "season = list(df['seasons'].unique())\n",
    "season"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "formal-tobacco",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_type = pn.widgets.Select(value='Winter', options=season, name='Seasons')\n",
    "s_type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "persistent-collection",
   "metadata": {},
   "source": [
    "Noticed what just happened, using `pn.widgets.Select` we assigned a default `value` to a widget that allow users to choose a property from a list of `options`. We also gave it a name to make it even more intuitive.\n",
    "\n",
    "Let's create another one for the day of the week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ff02e12-f552-4674-ace4-36b2f093df72",
   "metadata": {},
   "outputs": [],
   "source": [
    "day_of_week = list(df.day_of_week.unique())\n",
    "weekday = pn.widgets.Select(value='Friday', options=day_of_week, name='Day of Week')\n",
    "weekday"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "consistent-circumstances",
   "metadata": {},
   "source": [
    "While we will only use the `pn.widgets.Select` and `pn.interact` functions for this notebook, there are other amazing widgets that you should definitely explore whenever you can at [panel widgets](https://panel.holoviz.org/user_guide/Widgets.html).\n",
    "\n",
    "Time for a quick exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "headed-filing",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "1. Pick any categorical column from the dataset.\n",
    "2. Get the unique values of such category.\n",
    "3. Create a widget, assign it to a variable called `my_widget` and then display it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "alleged-channels",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac1f1f0c-65b3-4a27-927b-ab4463171de8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "empty-queensland",
   "metadata": {},
   "source": [
    "### The Table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "quiet-plenty",
   "metadata": {},
   "source": [
    "Tables are exactly that, a view whatever data we have but with an interactive component to it. We will create one for the numerical variables in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subsequent-conjunction",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_vars = ['temperaturec', 'humidity', 'wind_speed_ms', 'visibility_10m', 'dew_point_temperaturec',\n",
    "            'solar_radiation_mjm2', 'rainfallmm', 'snowfall_cm']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-brighton",
   "metadata": {},
   "source": [
    "Let's now create a double mask for the seasons of the year."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wicked-change",
   "metadata": {},
   "outputs": [],
   "source": [
    "seasons_mask = (df['seasons'] == 'Winter')\n",
    "data = df[seasons_mask].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genetic-bhutan",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_group = data[num_vars].mean().to_frame(name='vals').reset_index()\n",
    "data_group.columns = ['Numerical Vars', 'Average Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "neural-timer",
   "metadata": {},
   "outputs": [],
   "source": [
    "table = hv.Table(data_group).opts(width=350, height=250)\n",
    "table"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "enabling-performer",
   "metadata": {},
   "source": [
    "Now let's wrap our process again into a function that will take only one parameter, the season type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "standing-canyon",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pn.depends(s_type.param.value)\n",
    "def get_num_table(s_type):\n",
    "    \n",
    "    mask = (df['seasons'] == s_type)\n",
    "    data = df[mask].copy()\n",
    "    data_group = data[num_vars].mean().to_frame(name='vals').reset_index()\n",
    "    data_group.columns = ['Numerical Vars', 'Average Score']\n",
    "    \n",
    "    return hv.Table(data_group).opts(width=300, height=180, bgcolor='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "durable-supply",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you ever with holoviews functions, make sure to use\n",
    "# hv.help(opts.Table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50769b6",
   "metadata": {},
   "source": [
    "We can examine our function and widget together in the browser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pregnant-label",
   "metadata": {},
   "outputs": [],
   "source": [
    "pn.Column(s_type, get_num_table).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "japanese-coalition",
   "metadata": {},
   "source": [
    "### The Whiskers ðŸ±"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "boolean-sacrifice",
   "metadata": {},
   "source": [
    "Box plots are incredibly useful for telling us the key descriptive statistics of a variable. Mainly, they provide us with the minimum value, the interquartile, the median, the maximum, and a view of the outliers, if any.\n",
    "\n",
    "Let's plot the distribution of bikes rented per hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "russian-destination",
   "metadata": {},
   "outputs": [],
   "source": [
    "weekday_mask = (df['day_of_week'] == 'Monday') & (df['seasons'] == \"Winter\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stone-archive",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.loc[weekday_mask, ['rented_bike_count', 'hour']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-newport",
   "metadata": {},
   "outputs": [],
   "source": [
    "# since our label is a bit long we will add it to a variable\n",
    "label = \"Rented Bikes per Hour\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-helping",
   "metadata": {},
   "source": [
    "Let's now create our box plot using holoview function `hv.BoxWhisker`, which contains most of the parameters we are familiar with by now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-genetics",
   "metadata": {},
   "outputs": [],
   "source": [
    "bw = hv.BoxWhisker(data, 'hour', 'rented_bike_count', label=label)\n",
    "bw"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fifteen-petite",
   "metadata": {},
   "source": [
    "As we can see, our chart needs some options to be more useful, so let's give some of the ones we have already used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "periodic-reduction",
   "metadata": {},
   "outputs": [],
   "source": [
    "bw.opts(box_fill_color='#D5E051', box_line_color='#5F6062', width=700, height=350, box_line_width=1,\n",
    "        whisker_color='#FFFFFF', xrotation=25, bgcolor='#5F6062', labelled=[], outlier_color='#FFFFFF')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-world",
   "metadata": {},
   "source": [
    "Let's now package eveything in a function that will change given the net promoter score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f36593-f6ca-4aff-a346-7d4cf6595f0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretty_options = dict(box_fill_color='#D5E051', box_line_color='#5F6062', width=850, height=350, box_line_width=1, \n",
    "                      whisker_color='#FFFFFF', xrotation=25, bgcolor='#5F6062', outlier_color='#FFFFFF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pressing-ebony",
   "metadata": {},
   "outputs": [],
   "source": [
    "@pn.depends(s_type.param.value, weekday.param.value)\n",
    "def cat_whisker(s_type, weekday):\n",
    "    \n",
    "    mask = (df['seasons'] == s_type) & (df['day_of_week'] == weekday)\n",
    "    data = df.loc[mask, ['hour', 'rented_bike_count']].copy()\n",
    "    label = f\"(2-Week Stay) Price Range per Property Type with {weekday} Reviews\"\n",
    "    \n",
    "    return hv.BoxWhisker(data, 'hour', 'rented_bike_count', label=label).opts(**pretty_options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "middle-absorption",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pn.Column(s_type, weekday, cat_whisker).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dramatic-hygiene",
   "metadata": {},
   "source": [
    "### Exercise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "neural-showcase",
   "metadata": {},
   "source": [
    "Create a box and whisker plot showing the distribution of the temperature column. You can use the months for your x axis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "thrown-celebration",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "referenced-methodology",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-conversion",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "artistic-chocolate",
   "metadata": {},
   "source": [
    "### Dots as Bars"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-budget",
   "metadata": {},
   "source": [
    "In this section, we want to use a variation of a bar chart in order to detect whether there are any differences between bikes rented at the start of the mont versus the rest of the days."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-integral",
   "metadata": {},
   "outputs": [],
   "source": [
    "group = df.groupby(['month', 'is_month_start'])['rented_bike_count'].mean().reset_index()\n",
    "group.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "shaped-scott",
   "metadata": {},
   "outputs": [],
   "source": [
    "is_month_start = group[group['is_month_start'] == True]\n",
    "is_month_else = group[group['is_month_start'] == False]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rising-routine",
   "metadata": {},
   "source": [
    "We then create our first scatter and load it with options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e6e3a8-217a-4fa1-970e-ad7d02b18aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "optional_settings = dict(width=500, show_grid=True, height=420, invert_axes=True, size=7, tools=['hover'],\n",
    "                         legend_position='bottom_right', toolbar='right', labelled=[], \n",
    "                         title=\"Average # of Bikes Rented at the Start/End of the Month\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "conventional-scientist",
   "metadata": {},
   "outputs": [],
   "source": [
    "dots1 = (hv.Scatter(is_month_start, 'month', 'rented_bike_count', label='Start')\n",
    "           .sort('rented_bike_count').opts(**optional_settings, color='#D5E051'))\n",
    "dots1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5eb1647-4c92-4e79-9ca9-40b326b097a2",
   "metadata": {},
   "source": [
    "Luckily, the second scatter does not need as many functions as we will overlay it on top of the sorted first right away."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "apart-medicare",
   "metadata": {},
   "outputs": [],
   "source": [
    "dots2 = (hv.Scatter(is_month_else, 'month', 'rented_bike_count', label='Rest')\n",
    "           .opts(**optional_settings, color='#D8DEE9'))\n",
    "dots2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a801dbd-52f6-4eb9-a7a3-b863593fb1fb",
   "metadata": {},
   "source": [
    "We can overlay the two with the `*` sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "blessed-mobility",
   "metadata": {},
   "outputs": [],
   "source": [
    "dot_combo = (dots1 * dots2)\n",
    "dot_combo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fancy-officer",
   "metadata": {},
   "source": [
    "### The Title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "disciplinary-james",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = pn.pane.Markdown(f\"# Bikes in Seoul\", style={\"color\": \"#2E3440\"}, width=500, height=50,\n",
    "                        sizing_mode=\"stretch_width\", margin=(0,0,0,5))\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-missouri",
   "metadata": {},
   "outputs": [],
   "source": [
    "img = pn.pane.PNG(\"https://media4.giphy.com/media/ycMyB9MMSohHR6kOFe/giphy.gif\", height=50, sizing_mode=\"fixed\", align=\"center\")\n",
    "img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "wireless-consideration",
   "metadata": {},
   "outputs": [],
   "source": [
    "title = pn.Row(text, img, background=\"#D8DEE9\", sizing_mode='scale_both', max_height=60, min_width=800)\n",
    "title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cloudy-spread",
   "metadata": {},
   "source": [
    "### Putting it all Together"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ongoing-rates",
   "metadata": {},
   "source": [
    "The most important piece of this part is the sizing of your dashboard or app. Something that works well is to either grab a pen and paper and draw what first what you envision as a dashboard before creating one. While you draw boxes, it is also beneficial play around with the width and the height to each box in your visualization, that way you know how to set proper dimensions later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "continued-wrestling",
   "metadata": {},
   "outputs": [],
   "source": [
    "c1  = pn.Column(s_type, pn.Spacer(height=10), weekday, table, min_width=800, height=400, align='center')\n",
    "c1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-capability",
   "metadata": {},
   "outputs": [],
   "source": [
    "c2 = pn.Row(cat_whisker, height=360, align='center', min_width=650)\n",
    "c2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-skirt",
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = pn.Row(dot_combo, pn.Spacer(width=10), c1, sizing_mode='fixed', align='center', width=950, height=400)\n",
    "r1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "covered-switzerland",
   "metadata": {},
   "outputs": [],
   "source": [
    "dashboard = pn.Column(title, pn.Spacer(height=15), r1, pn.Spacer(height=20), c2, background='#5F6062',\n",
    "                      sizing_mode='scale_both', align='center', min_height=1000, min_width=1050)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inclusive-platinum",
   "metadata": {},
   "outputs": [],
   "source": [
    "dashboard.show(threaded=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d6ef34f-012e-41cd-9d05-be07f2c16275",
   "metadata": {
    "tags": []
   },
   "source": [
    "### The Theme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108f0c4a-37e7-4e83-8806-1932e5693bdf",
   "metadata": {},
   "source": [
    "We can also create and use custom themes to make our dashboards look much prettier. Here is the [inspiration](https://bigbookofdashboards.com/images/Figure13.png) for this one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "careful-settlement",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bokeh.themes.theme import Theme\n",
    "\n",
    "theme = Theme(\n",
    "    json={\n",
    "    'attrs' : {\n",
    "        'Figure' : {\n",
    "            'background_fill_color': '#5F6062',\n",
    "            'border_fill_color': '#5F6062',\n",
    "            'outline_line_color': '#5F6062',\n",
    "        },\n",
    "        'Grid': {\n",
    "            'grid_line_dash': [6, 4],\n",
    "            'grid_line_alpha': .3,\n",
    "        },\n",
    "\n",
    "        'Axis': {\n",
    "            'major_label_text_color': '#D5E051',\n",
    "            'axis_label_text_color': '#D5E051',\n",
    "            'major_tick_line_color': '#D5E051',\n",
    "            'minor_tick_line_color': '#D5E051',\n",
    "            'axis_line_color': \"#D5E051\"\n",
    "        },\n",
    "        'Title': {\n",
    "            'text_color': '#FFFFFF'\n",
    "        }\n",
    "    }\n",
    "})\n",
    "\n",
    "hv.renderer('bokeh').theme = theme"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "297a3deb-847f-48cc-acc9-51f24295d7e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Saving it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aware-ghana",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dashboard.save('analytics/interactive_dash.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hazardous-people",
   "metadata": {},
   "source": [
    "### Analytics Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "advanced-operator",
   "metadata": {},
   "source": [
    "1. Widgets can be created for categories or discrete numbers and floats, here we have used mainly categories\n",
    "2. Start building your visualisations step by step and once you have an MVP, focus onn wrapping the operations in functions\n",
    "1. Using the tools chosen for the tutorial, interactive charts require functions that are tied to widgets\n",
    "2. These functions get computed every time a value changes and the visual display gets updated\n",
    "3. The larger the dataset the longer repeated computation might take"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Science"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by getting our directory ready and evaluating our project.\n",
    "\n",
    "```bash\n",
    "pip install cookiecutter\n",
    "\n",
    "# and then\n",
    "cookiecutter https://github.com/drivendata/cookiecutter-data-science\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now change a few things in our `src` directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "\n",
    "rm -rf data_sci/src/*\n",
    "mkdir data_sci/src/local_flows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![metaflow](https://repository-images.githubusercontent.com/209120637/00b39080-1ddc-11ea-8710-59b484540700)\n",
    "\n",
    "What is Metaflow?\n",
    "> \"Metaflow makes it quick and easy to build and manage real-life data science projects.\" ~ [metaflow.org](metaflow.org)\n",
    "\n",
    "The best way to talk about metaflow is by using it so let's get to it. :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile data_sci/src/local_flows/first_flow.py\n",
    "\n",
    "from metaflow import FlowSpec, step, card\n",
    "\n",
    "class SingleFlow(FlowSpec):\n",
    "    \"\"\"\n",
    "    train multiple tree based methods\n",
    "    \"\"\"\n",
    "    @card \n",
    "    @step\n",
    "    def start(self):\n",
    "\n",
    "        import pandas as pd\n",
    "        from pathlib import Path\n",
    "\n",
    "        self.path = Path().cwd().parent\n",
    "\n",
    "        self.df = pd.read_parquet(self.path/\"data/processed/train.parquet\")\n",
    "        self.X_train = self.df.drop('rented_bike_count', axis=1)\n",
    "        self.y_train = self.df[\"rented_bike_count\"]\n",
    "        self.next(self.rf_model)\n",
    "    \n",
    "    @step\n",
    "    def rf_model(self):\n",
    "\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "        self.rf = RandomForestRegressor(\n",
    "            n_estimators=300, max_features=0.2, min_samples_leaf=1, verbose=1,\n",
    "            random_state=42, n_jobs=-1, oob_score=True\n",
    "        )\n",
    "\n",
    "        self.rf.fit(self.X_train, self.y_train)\n",
    "        self.next(self.save_model)\n",
    "\n",
    "    @step\n",
    "    def save_model(self):\n",
    "        \n",
    "        import pickle\n",
    "        \n",
    "        self.model_path = self.path/\"models\"\n",
    "        with open(self.model_path/\"rf_model.pkl\", \"wb\") as mod:\n",
    "            pickle.dump(self.rf, mod)\n",
    "\n",
    "        self.next(self.end)\n",
    "        \n",
    "    @step\n",
    "    def end(self):\n",
    "        print('Training Donr')\n",
    "        print(self.rf)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    SingleFlow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before running the flow, make sure you have opened a terminal and switched directories to `data_sci/src`. From there, let's run the following line.\n",
    "\n",
    "```python\n",
    "python local_flows/first_flow.py run\n",
    "```\n",
    "\n",
    "Let's walk through what just happened.\n",
    "\n",
    "Now let's run our flow and collect and evaluate our card.\n",
    "\n",
    "```python\n",
    "python local_flows/first_flow.py run --with card\n",
    "\n",
    "python local_flows/first_flow.py card view start\n",
    "```\n",
    "\n",
    "Now that we have a bit more information about how metaflow works, let's compare different machine learning models for our work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile data_sci/src/local_flows/multi_flow.py\n",
    "\n",
    "from metaflow import FlowSpec, step, card\n",
    "\n",
    "class ComplexFlow(FlowSpec):\n",
    "    \n",
    "    @card \n",
    "    @step\n",
    "    def start(self):\n",
    "\n",
    "        import pandas as pd\n",
    "        from pathlib import Path\n",
    "\n",
    "        self.path = Path().cwd().parent\n",
    "\n",
    "        #Load dataset\n",
    "        self.df = pd.read_parquet(self.path/\"data/processed/train.parquet\")\n",
    "        self.X_train = self.df.drop('rented_bike_count', axis=1)\n",
    "        self.y_train = self.df[\"rented_bike_count\"]\n",
    "        self.next(self.rf_model, self.lgbm_model, self.xgb_model, self.cat_model)\n",
    "    \n",
    "                \n",
    "    @step\n",
    "    def rf_model(self):\n",
    "\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        \n",
    "        self.reg = RandomForestRegressor(\n",
    "            n_estimators=300, max_features=0.2, min_samples_leaf=1, verbose=1,\n",
    "            random_state=42, n_jobs=-1, oob_score=True\n",
    "        )\n",
    "\n",
    "        self.reg.fit(self.X_train.values, self.y_train.values)\n",
    "        self.scores = self.reg.score(self.X_train.values, self.y_train.values)\n",
    "        self.next(self.model_evaluation)\n",
    "\n",
    "    @step\n",
    "    def lgbm_model(self):\n",
    "\n",
    "        from lightgbm import LGBMRegressor\n",
    "\n",
    "        self.reg = LGBMRegressor(n_estimators=200, random_state=42)\n",
    "        self.reg.fit(self.X_train.values, self.y_train.values)\n",
    "        self.scores = self.reg.score(self.X_train.values, self.y_train.values)\n",
    "\n",
    "        self.next(self.model_evaluation)\n",
    "\n",
    "    @step\n",
    "    def cat_model(self):\n",
    "\n",
    "        from catboost import CatBoostRegressor\n",
    "        \n",
    "        self.reg = CatBoostRegressor(n_estimators=200, random_state=42)\n",
    "        self.reg.fit(self.X_train.values, self.y_train.values)\n",
    "        self.scores = self.reg.score(self.X_train.values, self.y_train.values)\n",
    "\n",
    "        self.next(self.model_evaluation)\n",
    "\n",
    "    @step\n",
    "    def xgb_model(self):\n",
    "\n",
    "        from xgboost import XGBRFRegressor\n",
    "        \n",
    "        self.reg = XGBRFRegressor(n_estimators=200, random_state=42)\n",
    "        self.reg.fit(self.X_train.values, self.y_train.values)\n",
    "        self.scores = self.reg.score(self.X_train.values, self.y_train.values)\n",
    "\n",
    "        self.next(self.model_evaluation)\n",
    "           \n",
    "    @step\n",
    "    def model_evaluation(self, inputs):\n",
    "\n",
    "        import pandas as pd\n",
    "\n",
    "        self.results = [(inp.reg.__repr__(), inp.reg, inp.scores) for inp in inputs]\n",
    "        self.df_results = pd.DataFrame(self.results, columns=[\"name\", \"model\", \"scores\"])\n",
    "\n",
    "        self.best_score = self.df_results[\"scores\"].max()\n",
    "        self.best_model = self.df_results.loc[self.df_results[\"scores\"] == self.best_score, \"model\"]\n",
    "        self.next(self.end)\n",
    "        \n",
    "    @step\n",
    "    def end(self):\n",
    "\n",
    "        print(f'Scores:\\n{self.df_results.to_markdown()}')\n",
    "        print(f'Best model: {self.best_model.__repr__()}')\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    ComplexFlow()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "python local_flows/multi_flow.py run --with card\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we compared different tree-based frameworks to see which would give us the best performance off the bat, and we did so by adding three additional functions to our flow. Let's evaluate what the card has to show us.\n",
    "\n",
    "```python\n",
    "python local_flows/multi_flow.py card view start\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Beyond Workflows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Experiment Tracking\n",
    "2. Feature Stores\n",
    "3. Testing for data, code, properties, and more\n",
    "4. MLE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.8 ('wflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "873b278e3e2ac8ef58ae749cda81862ba3351740a62f1cd0af2e10695a326178"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
